{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550bec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc433269",
   "metadata": {},
   "source": [
    "Agent Observability - Logs, Traces & Metrics\n",
    "\n",
    "Today, you'll learn:\n",
    "\n",
    "How to add observability to the agent you've built and\n",
    "How to evaluate if the agents are working as expected\n",
    "In this notebook, we'll focus on the first part - Agent Observability!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa984f2",
   "metadata": {},
   "source": [
    "What is Agent Observability?¬∂\n",
    "üö® The challenge: Unlike traditional software that fails predictably, AI agents can fail mysteriously. Example:\n",
    "\n",
    "User: \"Find quantum computing papers\"\n",
    "Agent: \"I cannot help with that request.\"\n",
    "You: üò≠ WHY?? Is it the prompt? Missing tools? API error?\n",
    "üí° The Solution: Agent observability gives you complete visibility into your agent's decision-making process. You'll see exactly what prompts are sent to the LLM, which tools are available, how the model responds, and where failures occur.\n",
    "\n",
    "DEBUG Log: LLM Request shows \"Functions: []\" (no tools!)\n",
    "You: üéØ Aha! Missing google_search tool - easy fix!\n",
    "Foundational pillars of Agent Observability\n",
    "Logs: A log is a record of a single event, telling you what happened at a specific moment.\n",
    "Traces: A trace connects the logs into a single story, showing you why a final result occurred by revealing the entire sequence of steps.\n",
    "Metrics: Metrics are the summary numbers (like averages and error rates) that tell you how well the agent is performing overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cadb7",
   "metadata": {},
   "source": [
    " 1.3: Set up logging and cleanup old files¬∂\n",
    "Let's configure logging for our debugging session. The following cell makes sure we also capture other log levels, like DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96492e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Clean up any previous logs\n",
    "for log_file in [\"logger.log\", \"web.log\", \"tunnel.log\"]:\n",
    "    if os.path.exists(log_file):\n",
    "        os.remove(log_file)\n",
    "        print(f\"üßπ Cleaned up {log_file}\")\n",
    "\n",
    "# Configure logging with DEBUG log level.\n",
    "logging.basicConfig(\n",
    "    filename=\"logger.log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(filename)s:%(lineno)s %(levelname)s:%(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051dd8f",
   "metadata": {},
   "source": [
    " Section 2: Hands-On Debugging with ADK Web UI\n",
    "2.1: Create a \"Research Paper Finder\" Agent\n",
    "Our goal: Build a research paper finder agent that helps users find academic papers on any topic.\n",
    "\n",
    "But first, let's intentionally create an incorrect version of the agent to practice debugging! We'll start by creating a new agent folder using the adk create CLI command.\n",
    "\n",
    "\n",
    "Agent definition\n",
    "Next, let's create our root agent.\n",
    "\n",
    "We'll configure it as an LlmAgent, give it a name, model and instruction.\n",
    "The root_agent gets the user prompt and delegates the search to the google_search_agent.\n",
    "Then, the agent uses the count_papers tool to count the number of papers returned.\n",
    "üëâ Pay attention to the root agent's instructions and the count_papers tool parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2dcf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent created in d:\\5Days_Google_AI_course\\day_4\\research-agent:\n",
      "- .env\n",
      "- __init__.py\n",
      "- agent.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!adk create research-agent --model gemini-2.5-flash-lite --api_key $GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59ddd3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting research-agent/agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile research-agent/agent.py\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.tools.agent_tool import AgentTool\n",
    "from google.adk.tools.google_search_tool import google_search\n",
    "\n",
    "from google.genai import types\n",
    "from typing import List\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "# ---- Intentionally pass incorrect datatype - `str` instead of `List[str]` ----\n",
    "def count_papers(papers: str):\n",
    "    \"\"\"\n",
    "    This function counts the number of papers in a list of strings.\n",
    "    Args:\n",
    "      papers: A list of strings, where each string is a research paper.\n",
    "    Returns:\n",
    "      The number of papers in the list.\n",
    "    \"\"\"\n",
    "    return len(papers)\n",
    "\n",
    "\n",
    "# Google Search agent\n",
    "google_search_agent = LlmAgent(\n",
    "    name=\"google_search_agent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    description=\"Searches for information using Google search\",\n",
    "    instruction=\"\"\"Use the google_search tool to find information on the given topic. Return the raw search results.\n",
    "    If the user asks for a list of papers, then give them the list of research papers you found and not the summary.\"\"\",\n",
    "    tools=[google_search]\n",
    ")\n",
    "\n",
    "\n",
    "# Root agent\n",
    "root_agent = LlmAgent(\n",
    "    name=\"research_paper_finder_agent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    instruction=\"\"\"Your task is to find research papers and count them. \n",
    "\n",
    "    You MUST ALWAYS follow these steps:\n",
    "    1) Find research papers on the user provided topic using the 'google_search_agent'. \n",
    "    2) Then, pass the papers to 'count_papers' tool to count the number of papers returned.\n",
    "    3) Return both the list of research papers and the total number of papers.\n",
    "    \"\"\",\n",
    "    tools=[AgentTool(agent=google_search_agent), count_papers]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998b920",
   "metadata": {},
   "source": [
    "2.2: Run the agent\n",
    "Let's now run our agent with the adk web --log_level DEBUG CLI command.\n",
    "\n",
    "üìç The key here is --log_level DEBUG - this shows us:\n",
    "\n",
    "Full LLM Prompts: The complete request sent to the language model, including system instructions, history, and tools.\n",
    "Detailed API responses from services.\n",
    "Internal state transitions and variable values.\n",
    "Other log levels include: INFO, ERROR and WARNING.\n",
    "\n",
    "Get the proxied URL to access the ADK web UI in the Kaggle Notebooks environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb5588d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_prefix = \"http://localhost:8000\"  # Local backend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778402be",
   "metadata": {},
   "source": [
    "Now you can start the ADK web UI with the --log_level parameter.\n",
    "\n",
    "üëâ Note: The following cell will not \"complete\", but will remain running and serving the ADK web UI until you manually stop the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2db60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!adk web --log_level DEBUG --url_prefix {url_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f540d2",
   "metadata": {},
   "source": [
    "Once the ADK web UI starts, open the proxy link using the button in the previous cell.\n",
    "\n",
    "As you start chatting with the agent, you should see the DEBUG logs appear in the output cell below!\n",
    "\n",
    "‚ÄºÔ∏è IMPORTANT: DO NOT SHARE THE PROXY LINK with anyone - treat it as sensitive data as it contains your authentication token in the URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac9f85",
   "metadata": {},
   "source": [
    "\n",
    "üìù 2.3: Test the agent in ADK web UI\n",
    "üëâ Do: In the ADK web UI\n",
    "Select \"research-agent\" from the dropdown in the top-left.\n",
    "In the chat interface, type: Find latest quantum computing papers\n",
    "Send the message and observe the response. The agent should return a list of research papers and their count.\n",
    "It looks like our agent works and we got a response! ü§î But wait, isn't the count of papers unusually large? Let's look at the logs and trace.\n",
    "\n",
    "üëâ Do: Events tab - Traces in detail\n",
    "In the web UI, click the \"Events\" tab on the left sidebar\n",
    "You'll see a chronological list of all agent actions\n",
    "Click on any event to expand its details in the bottom panel\n",
    "Try clicking the \"Trace\" button to see timing information for each step.\n",
    "Click the execute_tool count_papers span. You'll see that the function call to count_papers returns the large number as the response.\n",
    "Let's look at what was passed as input to this function.\n",
    "Find the call_llm span corresponding to the count_papers function call.\n",
    "üëâ Do: Inspect the Function call in Events:\n",
    "Click on the specific span to open the Events tab.\n",
    "Examine the function_call, focusing on the papers argument.\n",
    "Notice that root_agent passes the list of papers as a str instead of a List[str] - there's our bug!\n",
    "Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045842d2",
   "metadata": {},
   "source": [
    "\n",
    "2.4: Your Turn - fix it! üëæ\n",
    "Update the datatype of the papers argument in the count_papers tool to a List[str] and rerun the adk web command!\n",
    "\n",
    "‚ÄºÔ∏è Stop the ADK web UI üõë\n",
    "In order to run cells in the remainder of this notebook, please stop the running cell where you started adk web in Section 3.1.\n",
    "\n",
    "Otherwise that running cell will block / prevent other cells from running as long as the ADK web UI is running.\n",
    "\n",
    "2.5: Debug through local Logs\n",
    "Optionally, you can also examine the local DEBUG logs to find the root cause. Run the following cell to print the contents of the log file. Look for detailed logs like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1cc072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Examining web server logs for debugging clues...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Examining web server logs for debugging clues...\\n\")\n",
    "!cat logger.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aafefd",
   "metadata": {},
   "source": [
    "other Observability questions you can now answer from logs and adk web:\n",
    "\n",
    "Efficiency: Is the agent making optimal tool choices?\n",
    "Reasoning Quality: Are the prompts well-structured and context-appropriate?\n",
    "Performance: Look at the traces to identify which steps take the longest?\n",
    "Failure Diagnosis: When something goes wrong, where exactly did it fail?\n",
    "Key Learning: Core debugging pattern: symptom ‚Üí logs ‚Üí root cause ‚Üí fix.\n",
    "\n",
    "Debugging Victory: You just went from \"Agent mysteriously failed\" to \"I know exactly why and how to fix it!\" This is the power of observability!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626aab6a",
   "metadata": {},
   "source": [
    "üßë‚Äçüíª Section 3: Logging in production¬∂\n",
    "üéØ Great! You can now debug agent failures using ADK web UI and DEBUG logs.\n",
    "\n",
    "But what happens when you move beyond development? Real-world scenarios where you need to move beyond the web UI:\n",
    "\n",
    "‚ùå Problem 1: Production Deployment\n",
    "\n",
    "You: \"Let me open the ADK web UI to check why the agent failed\"\n",
    "DevOps: \"Um... this is a production server. No web UI access.\"\n",
    "You: üò± \"How do I debug production issues?\"\n",
    "‚ùå Problem 2: Automated Systems\n",
    "\n",
    "You: \"The agent runs 1000 times per day in our pipeline\"\n",
    "Boss: \"Which runs are slow? What's our success rate?\"\n",
    "You: üò∞ \"I'd have to manually check the web UI 1000 times...\"\n",
    "üí° The Solution:\n",
    "\n",
    "We need a way to capture observability data or in other words, add logs to our code.\n",
    "\n",
    "üëâ In traditional software development, this is done by adding log statements in Python functions - and agents are no different! We need to add log statements to our agent and a common approach is to add log statements to Plugins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e2b38",
   "metadata": {},
   "source": [
    "\n",
    "3.1: How to add logs for production observability?\n",
    "A Plugin is a custom code module that runs automatically at various stages of your agent's lifecycle. Plugins are composed of \"Callbacks\" which provide the hooks to interrupt an agent's flow. Think of it like this:\n",
    "\n",
    "Your agent workflow: User message ‚Üí Agent thinks ‚Üí Calls tools ‚Üí Returns response\n",
    "Plugin hooks into this: Before agent starts ‚Üí After tool runs ‚Üí When LLM responds ‚Üí etc.\n",
    "Plugin contains your custom code: Logging, monitoring, security checks, caching, etc.\n",
    "image.png\n",
    "\n",
    "Callbacks\n",
    "Callbacks are the atomic components inside a Plugin - these are just Python functions that run at specific points in an agent's lifecycle! Callbacks are grouped together to create a Plugin.\n",
    "\n",
    "There are different kinds of callbacks such as:\n",
    "\n",
    "before/after_agent_callbacks - runs before/after an agent is invoked\n",
    "before/after_tool_callbacks - runs before/after a tool is called\n",
    "before/after_model_callbacks - similarly, runs before/after the LLM model is called\n",
    "on_model_error_callback - which runs when a model error is encountered\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466ffab",
   "metadata": {},
   "source": [
    "\n",
    "3.2: To make things more concrete, what does a Plugin look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468a7c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- EXAMPLE PLUGIN - DOES NOTHING ----- \n"
     ]
    }
   ],
   "source": [
    "print(\"----- EXAMPLE PLUGIN - DOES NOTHING ----- \")\n",
    "\n",
    "import logging\n",
    "from google.adk.agents.base_agent import BaseAgent\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models.llm_request import LlmRequest\n",
    "from google.adk.plugins.base_plugin import BasePlugin\n",
    "\n",
    "\n",
    "# Applies to all agent and model calls\n",
    "class CountInvocationPlugin(BasePlugin):\n",
    "    \"\"\"A custom plugin that counts agent and tool invocations.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the plugin with counters.\"\"\"\n",
    "        super().__init__(name=\"count_invocation\")\n",
    "        self.agent_count: int = 0\n",
    "        self.tool_count: int = 0\n",
    "        self.llm_request_count: int = 0\n",
    "\n",
    "    # Callback 1: Runs before an agent is called. You can add any custom logic here.\n",
    "    async def before_agent_callback(\n",
    "        self, *, agent: BaseAgent, callback_context: CallbackContext\n",
    "    ) -> None:\n",
    "        \"\"\"Count agent runs.\"\"\"\n",
    "        self.agent_count += 1\n",
    "        logging.info(f\"[Plugin] Agent run count: {self.agent_count}\")\n",
    "\n",
    "    # Callback 2: Runs before a model is called. You can add any custom logic here.\n",
    "    async def before_model_callback(\n",
    "        self, *, callback_context: CallbackContext, llm_request: LlmRequest\n",
    "    ) -> None:\n",
    "        \"\"\"Count LLM requests.\"\"\"\n",
    "        self.llm_request_count += 1\n",
    "        logging.info(f\"[Plugin] LLM request count: {self.llm_request_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fef8eb",
   "metadata": {},
   "source": [
    "Key insight: You register a plugin once on your runner, and it automatically applies to every agent, tool call, and LLM request in your system as per your definition. Read more about Plugin hooks here.\n",
    "\n",
    "You can follow along with the numbers in the diagram below to understand the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ca2ab",
   "metadata": {},
   "source": [
    "3.3: ADK's built-in LoggingPlugin\n",
    "But you don't have to define all the callbacks and plugins to capture standard Observability data in ADK. Instead, ADK provides a built-in LoggingPlugin that automatically captures all agent activity:\n",
    "\n",
    "üöÄ User messages and agent responses\n",
    "‚è±Ô∏è Timing data for performance analysis\n",
    "üß† LLM requests and responses for debugging\n",
    "üîß Tool calls and results\n",
    "‚úÖ Complete execution traces\n",
    "Agent definition\n",
    "Let's use the same agent from the previous demo - the Research paper finder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "from google.adk.tools.agent_tool import AgentTool\n",
    "from google.adk.tools.google_search_tool import google_search\n",
    "\n",
    "from google.genai import types\n",
    "from typing import List\n",
    "\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "\n",
    "def count_papers(papers: List[str]):\n",
    "    \"\"\"\n",
    "    This function counts the number of papers in a list of strings.\n",
    "    Args:\n",
    "      papers: A list of strings, where each string is a research paper.\n",
    "    Returns:\n",
    "      The number of papers in the list.\n",
    "    \"\"\"\n",
    "    return len(papers)\n",
    "\n",
    "\n",
    "# Google search agent\n",
    "google_search_agent = LlmAgent(\n",
    "    name=\"google_search_agent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    description=\"Searches for information using Google search\",\n",
    "    instruction=\"Use the google_search tool to find information on the given topic. Return the raw search results.\",\n",
    "    tools=[google_search],\n",
    ")\n",
    "\n",
    "# Root agent\n",
    "research_agent_with_plugin = LlmAgent(\n",
    "    name=\"research_paper_finder_agent\",\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    instruction=\"\"\"Your task is to find research papers and count them. \n",
    "   \n",
    "   You must follow these steps:\n",
    "   1) Find research papers on the user provided topic using the 'google_search_agent'. \n",
    "   2) Then, pass the papers to 'count_papers' tool to count the number of papers returned.\n",
    "   3) Return both the list of research papers and the total number of papers.\n",
    "   \"\"\",\n",
    "    tools=[AgentTool(agent=google_search_agent), count_papers],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a2942",
   "metadata": {},
   "source": [
    "3.4: Add LoggingPlugin to Runner\n",
    "The following code creates the InMemoryRunner. This is used to programmatically invoke the agent.\n",
    "\n",
    "To use LoggingPlugin in the above research agent, 1) Import the plugin 2) Add it when initializing the InMemoryRunner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b166f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.plugins.logging_plugin import (\n",
    "    LoggingPlugin,\n",
    ")  # <---- 1. Import the Plugin\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "\n",
    "runner = InMemoryRunner(\n",
    "    agent=research_agent_with_plugin,\n",
    "    plugins=[\n",
    "        LoggingPlugin()\n",
    "    ],  # <---- 2. Add the plugin. Handles standard Observability logging across ALL agents\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now run the agent using run_debug function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95149c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Running agent with LoggingPlugin...\")\n",
    "print(\"üìä Watch the comprehensive logging output below:\\n\")\n",
    "\n",
    "response = await runner.run_debug(\"Find recent papers on quantum computing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ea95d",
   "metadata": {},
   "source": [
    "üìä Summary¬∂\n",
    "‚ùì When to use which type of Logging?\n",
    "\n",
    "Development debugging? ‚Üí Use adk web --log_level DEBUG\n",
    "Common production observability? ‚Üí Use LoggingPlugin()\n",
    "Custom requirements? ‚Üí Build Custom Callbacks and Plugins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a10b15",
   "metadata": {},
   "source": [
    "4B\n",
    "EVALUATE YOUR AGENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f2151",
   "metadata": {},
   "source": [
    "Agent Evaluation¬∂\n",
    "Welcome to Day 4 of the Kaggle 5-day Agents course!\n",
    "\n",
    "In the previous notebook, we explored how to implement Observability in AI agents. This approach is primarily reactive; it comes into play after an issue has surfaced, providing the necessary data to debug and understand the root cause.\n",
    "\n",
    "In this notebook, we'll complement those observability practices with a proactive approach using Agent Evaluation. By continuously evaluating our agent's performance, we can catch any quality degradations much earlier!\n",
    "\n",
    "                            Observability + Agent Evaluation\n",
    "                            (reactive)      (proactive)\n",
    "What is Agent Evaluation?\n",
    "It is the systematic process of testing and measuring how well an AI agent performs across different scenarios and quality dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5073be",
   "metadata": {},
   "source": [
    "\n",
    "ü§ñ The story\n",
    "You've built a home automation agent. It works perfectly in your tests, so you launch it confidently...\n",
    "\n",
    "Week 1: üö® \"Agent turned on the fireplace when I asked for lights!\"\n",
    "Week 2: üö® \"Agent won't respond to commands in the guest room!\"\n",
    "Week 3: üö® \"Agent gives rude responses when devices are unavailable!\"\n",
    "The Problem: Standard testing ‚â† Evaluation\n",
    "\n",
    "Agents are different from traditional software:\n",
    "\n",
    "They are non-deterministic\n",
    "Users give unpredictable, ambiguous commands\n",
    "Small prompt changes cause dramatic behavior shifts and different tool calls\n",
    "To accommodate all these differences, agents need systematic evaluation, not just \"happy path\" testing. Which means assessing the agent's entire decision-making process - including the final response and the path it took to get the response (trajectory)!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fe6e7",
   "metadata": {},
   "source": [
    "Section 2: Create a Home Automation Agent¬∂\n",
    "Let's create the agent that will be the center of our evaluation story. This home automation agent seems perfect in basic tests but has hidden flaws we'll discover through comprehensive evaluation. Run the adk create CLI command to set up the project scaffolding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90886cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!adk create home_automation_agent --model gemini-2.5-flash-lite --api_key $GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5e1f7",
   "metadata": {},
   "source": [
    "Run the below cell to create the home automation agent.\n",
    "\n",
    "This agent uses a single set_device_status tool to control smart home devices. A device's status can only be ON or OFF. The agent's instruction is deliberately overconfident - it claims to control \"ALL smart devices\" and \"any device the user mentions\" - setting up the evaluation problems we'll discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile home_automation_agent/agent.py\n",
    "\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "# Configure Model Retry on errors\n",
    "retry_config = types.HttpRetryOptions(\n",
    "    attempts=5,  # Maximum retry attempts\n",
    "    exp_base=7,  # Delay multiplier\n",
    "    initial_delay=1,\n",
    "    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n",
    ")\n",
    "\n",
    "def set_device_status(location: str, device_id: str, status: str) -> dict:\n",
    "    \"\"\"Sets the status of a smart home device.\n",
    "\n",
    "    Args:\n",
    "        location: The room where the device is located.\n",
    "        device_id: The unique identifier for the device.\n",
    "        status: The desired status, either 'ON' or 'OFF'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming the action.\n",
    "    \"\"\"\n",
    "    print(f\"Tool Call: Setting {device_id} in {location} to {status}\")\n",
    "    return {\n",
    "        \"success\": True,\n",
    "        \"message\": f\"Successfully set the {device_id} in {location} to {status.lower()}.\"\n",
    "    }\n",
    "\n",
    "# This agent has DELIBERATE FLAWS that we'll discover through evaluation!\n",
    "root_agent = LlmAgent(\n",
    "    model=Gemini(model=\"gemini-2.5-flash-lite\", retry_options=retry_config),\n",
    "    name=\"home_automation_agent\",\n",
    "    description=\"An agent to control smart devices in a home.\",\n",
    "    instruction=\"\"\"You are a home automation assistant. You control ALL smart devices in the house.\n",
    "    \n",
    "    You have access to lights, security systems, ovens, fireplaces, and any other device the user mentions.\n",
    "    Always try to be helpful and control whatever device the user asks for.\n",
    "    \n",
    "    When users ask about device capabilities, tell them about all the amazing features you can control.\"\"\",\n",
    "    tools=[set_device_status],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17b1e3",
   "metadata": {},
   "source": [
    "Section 3: Interactive Evaluation with ADK Web UI¬∂\n",
    "3.1: Launch ADK Web UI\n",
    "Get the proxied URL to access the ADK web UI in the Kaggle Notebooks environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!adk web --url_prefix {url_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55aa46f",
   "metadata": {},
   "source": [
    "Once the ADK web UI starts, open the proxy link using the button in the previous cell.\n",
    "\n",
    "‚ÄºÔ∏è IMPORTANT: DO NOT SHARE THE PROXY LINK with anyone - treat it as sensitive data as it contains your authentication token in the URL.\n",
    "\n",
    "3.2: Create Your First \"Perfect\" Test Case\n",
    "üëâ Do: In the ADK web UI:\n",
    "\n",
    "Click the public URL above to open the ADK web UI\n",
    "Select \"home_automation_agent\" from the dropdown\n",
    "Have a normal conversation: Type Turn on the desk lamp in the office\n",
    "Agent responds correctly - controls device and confirms action\n",
    "üëâ Do: Save this as your first evaluation case:\n",
    "\n",
    "Navigate to the Eval tab on the right-hand panel\n",
    "Click Create Evaluation set and name it home_automation_tests\n",
    "In the home_automation_tests set, click the \">\" arrow and click Add current session\n",
    "Give it the case name basic_device_control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc319910",
   "metadata": {},
   "source": [
    "Run the Evaluation¬∂\n",
    "üëâ Do: Run your first evaluation\n",
    "\n",
    "Now, let's run the test case to see if the agent can replicate its previous success.\n",
    "\n",
    "In the Eval tab, make sure your new test case is checked.\n",
    "Click the Run Evaluation button.\n",
    "The EVALUATION METRIC dialog will appear. For now, leave the default values and click Start.\n",
    "The evaluation will run, and you should see a green Pass result in the Evaluation History. This confirms the agent's behavior matched the saved session.\n",
    "‚ÄºÔ∏è Understanding the Evaluation Metrics\n",
    "\n",
    "When you run evaluation, you'll see two key scores:\n",
    "\n",
    "Response Match Score: Measures how similar the agent's actual response is to the expected response. Uses text similarity algorithms to compare content. A score of 1.0 = perfect match, 0.0 = completely different.\n",
    "\n",
    "Tool Trajectory Score: Measures whether the agent used the correct tools with correct parameters. Checks the sequence of tool calls against expected behavior. A score of 1.0 = perfect tool usage, 0.0 = wrong tools or parameters.\n",
    "\n",
    "üëâ Do: Analyze a Failure\n",
    "\n",
    "Let's intentionally break the test to see what a failure looks like.\n",
    "\n",
    "In the list of eval cases, click the Edit (pencil) icon next to your test case.\n",
    "In the \"Final Response\" text box, change the expected text to something incorrect, like: The desk lamp is off.\n",
    "Save the changes and re-run the evaluation.\n",
    "This time, the result will be a red Fail. Hover your mouse over the \"Fail\" label. A tooltip will appear showing a side-by-side comparison of the Actual vs. Expected Output, highlighting exactly why the test failed (the final response didn't match). This immediate, detailed feedback is invaluable for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f421890",
   "metadata": {},
   "source": [
    "Create these scenarios in separate conversations:\n",
    "\n",
    "Ambiguous Commands: \"Turn on the lights in the bedroom\"\n",
    "\n",
    "Save as a new test case: ambiguous_device_reference\n",
    "Run evaluation - it likely passes but the agent might be confused\n",
    "Invalid Locations: \"Please turn off the TV in the garage\"\n",
    "\n",
    "Save as a new test case: invalid_location_test\n",
    "Run evaluation - the agent might try to control non-existent devices\n",
    "Complex Commands: \"Turn off all lights and turn on security system\"\n",
    "\n",
    "Save as a new test case: complex_multi_device_command\n",
    "Run evaluation - the agent might attempt operations beyond its capabilities\n",
    "The Problem You'll Discover: Even when tests \"pass,\" you can see the agent:\n",
    "\n",
    "Makes assumptions about devices that don't exist\n",
    "Gives responses that sound helpful but aren't accurate\n",
    "Tries to control devices it shouldn't have access to\n",
    "ü§î What am I missing?\n",
    "‚ùå Web UI Limitation: So far, we've seen how to create and evaluate test cases in the ADK web UI. The web UI is great for interactive test creation, but testing one conversation at a time doesn't scale.\n",
    "\n",
    "‚ùì The Question: How do I proactively detect regressions in my agent's performance?\n",
    "\n",
    "Let's answer that question in the next section!\n",
    "\n",
    "‚ÄºÔ∏è Stop the ADK web UI üõë\n",
    "In order to run cells in the remainder of this notebook, please stop the running cell where you started adk web in Section 3.1.\n",
    "\n",
    "Otherwise that running cell will block / prevent other cells from running as long as the ADK web UI is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b010fd",
   "metadata": {},
   "source": [
    "\n",
    "üìà Section 4: Systematic Evaluation\n",
    "Regression testing is the practice of re-running existing tests to ensure that new changes haven't broken previously working functionality.\n",
    "\n",
    "ADK provides two methods to do automatic regression and batch testing: using pytest and the adk eval CLI command. In this section, we'll use the CLI command. For more information on the pytest approach, refer to the links in the resource section at the end of this notebook.\n",
    "\n",
    "The following image shows the overall process of evaluation. At a high-level, there are four steps to evaluate:\n",
    "\n",
    "1) Create an evaluation configuration - define metrics or what you want to measure 2) Create test cases - sample test cases to compare against 3) Run the agent with test query 4) Compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4427f",
   "metadata": {},
   "source": [
    "4.1: Create evaluation configuration\n",
    "This optional file lets us define the pass/fail thresholds. Create test_config.json in the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d667281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Create evaluation configuration with basic criteria\n",
    "eval_config = {\n",
    "    \"criteria\": {\n",
    "        \"tool_trajectory_avg_score\": 1.0,  # Perfect tool usage required\n",
    "        \"response_match_score\": 0.8,  # 80% text similarity threshold\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"home_automation_agent/test_config.json\", \"w\") as f:\n",
    "    json.dump(eval_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Evaluation configuration created!\")\n",
    "print(\"\\nüìä Evaluation Criteria:\")\n",
    "print(\"‚Ä¢ tool_trajectory_avg_score: 1.0 - Requires exact tool usage match\")\n",
    "print(\"‚Ä¢ response_match_score: 0.8 - Requires 80% text similarity\")\n",
    "print(\"\\nüéØ What this evaluation will catch:\")\n",
    "print(\"‚úÖ Incorrect tool usage (wrong device, location, or status)\")\n",
    "print(\"‚úÖ Poor response quality and communication\")\n",
    "print(\"‚úÖ Deviations from expected behavior patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f9319",
   "metadata": {},
   "source": [
    "4.2: Create test cases¬∂\n",
    "This file (integration.evalset.json) will contain multiple test cases (sessions).\n",
    "\n",
    "This evaluation set can be created synthetically or from the conversation sessions in the ADK web UI.\n",
    "\n",
    "Tip: To persist the conversations from the ADK web UI, simply create an evalset in the UI and add the current session to it. All the conversations in that session will be auto-converted to an evalset and downloaded locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation test cases that reveal tool usage and response quality problems\n",
    "test_cases = {\n",
    "    \"eval_set_id\": \"home_automation_integration_suite\",\n",
    "    \"eval_cases\": [\n",
    "        {\n",
    "            \"eval_id\": \"living_room_light_on\",\n",
    "            \"conversation\": [\n",
    "                {\n",
    "                    \"user_content\": {\n",
    "                        \"parts\": [\n",
    "                            {\"text\": \"Please turn on the floor lamp in the living room\"}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"final_response\": {\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                \"text\": \"Successfully set the floor lamp in the living room to on.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"intermediate_data\": {\n",
    "                        \"tool_uses\": [\n",
    "                            {\n",
    "                                \"name\": \"set_device_status\",\n",
    "                                \"args\": {\n",
    "                                    \"location\": \"living room\",\n",
    "                                    \"device_id\": \"floor lamp\",\n",
    "                                    \"status\": \"ON\",\n",
    "                                },\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"eval_id\": \"kitchen_on_off_sequence\",\n",
    "            \"conversation\": [\n",
    "                {\n",
    "                    \"user_content\": {\n",
    "                        \"parts\": [{\"text\": \"Switch on the main light in the kitchen.\"}]\n",
    "                    },\n",
    "                    \"final_response\": {\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                \"text\": \"Successfully set the main light in the kitchen to on.\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"intermediate_data\": {\n",
    "                        \"tool_uses\": [\n",
    "                            {\n",
    "                                \"name\": \"set_device_status\",\n",
    "                                \"args\": {\n",
    "                                    \"location\": \"kitchen\",\n",
    "                                    \"device_id\": \"main light\",\n",
    "                                    \"status\": \"ON\",\n",
    "                                },\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25dcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write the test cases to the integration.evalset.json in our agent's root directory.\n",
    "import json\n",
    "\n",
    "with open(\"home_automation_agent/integration.evalset.json\", \"w\") as f:\n",
    "    json.dump(test_cases, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Evaluation test cases created\")\n",
    "print(\"\\nüß™ Test scenarios:\")\n",
    "for case in test_cases[\"eval_cases\"]:\n",
    "    user_msg = case[\"conversation\"][0][\"user_content\"][\"parts\"][0][\"text\"]\n",
    "    print(f\"‚Ä¢ {case['eval_id']}: {user_msg}\")\n",
    "\n",
    "print(\"\\nüìä Expected results:\")\n",
    "print(\"‚Ä¢ basic_device_control: Should pass both criteria\")\n",
    "print(\n",
    "    \"‚Ä¢ wrong_tool_usage_test: May fail tool_trajectory if agent uses wrong parameters\"\n",
    ")\n",
    "print(\n",
    "    \"‚Ä¢ poor_response_quality_test: May fail response_match if response differs too much\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fd941",
   "metadata": {},
   "source": [
    ".3: Run CLI Evaluation¬∂\n",
    "Execute the adk eval command, pointing it to your agent directory, the evalset, and the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b556e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Run this command to execute evaluation:\")\n",
    "!adk eval home_automation_agent home_automation_agent/integration.evalset.json --config_file_path=home_automation_agent/test_config.json --print_detailed_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaa420c",
   "metadata": {},
   "source": [
    "4.4: Analyzing sample evaluation results\n",
    "The command will run all test cases and print a summary. The --print_detailed_results flag provides a turn-by-turn breakdown of each test, showing scores and a diff for any failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdf065e",
   "metadata": {},
   "source": [
    "# Analyzing evaluation results - the data science approach\n",
    "print(\"üìä Understanding Evaluation Results:\")\n",
    "print()\n",
    "print(\"üîç EXAMPLE ANALYSIS:\")\n",
    "print()\n",
    "print(\"Test Case: living_room_light_on\")\n",
    "print(\"  ‚ùå response_match_score: 0.45/0.80\")\n",
    "print(\"  ‚úÖ tool_trajectory_avg_score: 1.0/1.0\")\n",
    "print()\n",
    "print(\"üìà What this tells us:\")\n",
    "print(\"‚Ä¢ TOOL USAGE: Perfect - Agent used correct tool with correct parameters\")\n",
    "print(\"‚Ä¢ RESPONSE QUALITY: Poor - Response text too different from expected\")\n",
    "print(\"‚Ä¢ ROOT CAUSE: Agent's communication style, not functionality\")\n",
    "print()\n",
    "print(\"üéØ ACTIONABLE INSIGHTS:\")\n",
    "print(\"1. Technical capability works (tool usage perfect)\")\n",
    "print(\"2. Communication needs improvement (response quality failed)\")\n",
    "print(\"3. Fix: Update agent instructions for clearer language or constrained response.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b0cd0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
